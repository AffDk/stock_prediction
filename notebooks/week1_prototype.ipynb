{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eee4d7c2",
   "metadata": {},
   "source": [
    "# Stock Price Prediction System - Week 1 Prototype\n",
    "\n",
    "## Overview\n",
    "This notebook implements a comprehensive stock price prediction system that combines:\n",
    "- **News Sentiment Analysis**: Using FinBERT to embed financial news articles\n",
    "- **Technical Indicators**: Historical stock data and financial markers  \n",
    "- **Machine Learning**: Neural network for price prediction\n",
    "- **Cloud Storage**: GCS for scalable data storage\n",
    "- **Cost Optimization**: Free tier utilization and efficient processing\n",
    "\n",
    "## Architecture\n",
    "```\n",
    "NewsAPI + yfinance → Feature Engineering → FinBERT Embeddings → PyTorch MLP → Price Prediction\n",
    "                        ↓\n",
    "                   Cloud Storage (GCS)\n",
    "```\n",
    "\n",
    "## Target\n",
    "Predict stock price 7 days in the future using:\n",
    "- News sentiment from preceding 7 days\n",
    "- Financial indicators and technical analysis\n",
    "- Cost-optimized cloud infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1712149a",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies\n",
    "\n",
    "First, let's set up our environment with UV package manager and install all required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f18357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we're in Colab\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# If in Colab, install packages\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running in Google Colab - installing packages...\")\n",
    "    !pip install yfinance newsapi-python transformers torch dask[complete] google-cloud-storage pandas pyarrow optuna scikit-learn matplotlib seaborn plotly\n",
    "else:\n",
    "    print(\"Running locally - assuming packages are installed via UV\")\n",
    "\n",
    "# Set up paths\n",
    "if 'google.colab' in sys.modules:\n",
    "    # In Colab, we'll work in a temporary directory\n",
    "    project_root = Path('/content/stock_prediction')\n",
    "    project_root.mkdir(exist_ok=True)\n",
    "    os.chdir(project_root)\n",
    "else:\n",
    "    # Local development - navigate to project root\n",
    "    project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "    os.chdir(project_root)\n",
    "    \n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570768f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "# Financial data libraries\n",
    "import yfinance as yf\n",
    "from newsapi import NewsApiClient\n",
    "\n",
    "# ML libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "\n",
    "# Transformers for FinBERT\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "# Cloud storage\n",
    "try:\n",
    "    from google.cloud import storage\n",
    "    GCS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Google Cloud Storage not available - will save locally only\")\n",
    "    GCS_AVAILABLE = False\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2228e38",
   "metadata": {},
   "source": [
    "## 2. Data Collection Configuration\n",
    "\n",
    "Set up configuration for our data collection including API keys, stock symbols, and date ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f070c60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # API Keys - UPDATE THESE WITH YOUR ACTUAL KEYS\n",
    "    'NEWS_API_KEY': 'XXX',  # Your NewsAPI key\n",
    "    'GCP_PROJECT_ID': 'your-gcp-project-id',  # Update with your GCP project ID\n",
    "    'GCP_BUCKET_NAME': 'stock-prediction-data',  # Update with your bucket name\n",
    "    \n",
    "    # Stock symbols for initial testing (5 major tech stocks)\n",
    "    'STOCK_SYMBOLS': ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'NVDA'],\n",
    "    \n",
    "    # Date range for data collection\n",
    "    'START_DATE': '2024-09-01',\n",
    "    'END_DATE': '2025-08-31',\n",
    "    \n",
    "    # For prototype, use smaller date range\n",
    "    'PROTOTYPE_START_DATE': (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d'),\n",
    "    'PROTOTYPE_END_DATE': datetime.now().strftime('%Y-%m-%d'),\n",
    "    \n",
    "    # Prediction parameters\n",
    "    'PREDICTION_HORIZON_DAYS': 7,  # Predict price 7 days in future\n",
    "    'NEWS_LOOKBACK_DAYS': 7,       # Use news from past 7 days\n",
    "    \n",
    "    # API rate limits\n",
    "    'NEWS_API_REQUESTS_PER_MINUTE': 50,\n",
    "    'NEWS_API_REQUESTS_PER_DAY': 1000,\n",
    "    \n",
    "    # Model parameters\n",
    "    'FINBERT_MODEL': 'ProsusAI/finbert',\n",
    "    'EMBEDDING_DIM': 768,\n",
    "    'FINANCIAL_FEATURES': 5,\n",
    "    'TOTAL_FEATURES': 773,  # 768 + 5\n",
    "    \n",
    "    # Training parameters\n",
    "    'BATCH_SIZE': 32,\n",
    "    'EPOCHS': 20,\n",
    "    'LEARNING_RATE': 0.001,\n",
    "    'TRAIN_TEST_SPLIT': 0.8,\n",
    "}\n",
    "\n",
    "# Stock-specific keywords for news filtering\n",
    "STOCK_KEYWORDS = {\n",
    "    'AAPL': ['apple', 'iphone', 'ipad', 'mac', 'tim cook', 'app store'],\n",
    "    'GOOGL': ['google', 'alphabet', 'search', 'youtube', 'android', 'sundar pichai'],\n",
    "    'MSFT': ['microsoft', 'windows', 'office', 'azure', 'satya nadella'],\n",
    "    'TSLA': ['tesla', 'elon musk', 'electric vehicle', 'ev', 'model s', 'model 3'],\n",
    "    'NVDA': ['nvidia', 'gpu', 'artificial intelligence', 'ai', 'gaming', 'data center']\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"Target stocks: {CONFIG['STOCK_SYMBOLS']}\")\n",
    "print(f\"Prototype date range: {CONFIG['PROTOTYPE_START_DATE']} to {CONFIG['PROTOTYPE_END_DATE']}\")\n",
    "print(f\"Full date range: {CONFIG['START_DATE']} to {CONFIG['END_DATE']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fd1962",
   "metadata": {},
   "source": [
    "## 3. News Data Scraping and Processing\n",
    "\n",
    "Implement NewsAPI integration to collect timestamped news articles for our target stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26969a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsCollector:\n",
    "    \"\"\"Collect news data from NewsAPI with rate limiting and filtering.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        self.client = NewsApiClient(api_key=api_key)\n",
    "        self.rate_limit_delay = 60 / CONFIG['NEWS_API_REQUESTS_PER_MINUTE']\n",
    "        \n",
    "    def collect_news_for_stock(self, symbol: str, start_date: str, end_date: str) -> List[Dict]:\n",
    "        \"\"\"Collect news articles for a specific stock symbol.\"\"\"\n",
    "        articles = []\n",
    "        keywords = STOCK_KEYWORDS.get(symbol, [])\n",
    "        \n",
    "        # Construct search query\n",
    "        query_terms = [symbol] + keywords[:3]  # Limit to avoid query length issues\n",
    "        query = f\"({' OR '.join(query_terms)})\"\n",
    "        \n",
    "        try:\n",
    "            print(f\"Collecting news for {symbol}...\")\n",
    "            \n",
    "            response = self.client.get_everything(\n",
    "                q=query,\n",
    "                from_param=start_date,\n",
    "                to=end_date,\n",
    "                language='en',\n",
    "                sort_by='publishedAt',\n",
    "                page_size=100\n",
    "            )\n",
    "            \n",
    "            if response['status'] == 'ok':\n",
    "                articles = response['articles']\n",
    "                print(f\"Found {len(articles)} articles for {symbol}\")\n",
    "            else:\n",
    "                print(f\"API error for {symbol}: {response}\")\n",
    "                \n",
    "            # Rate limiting\n",
    "            time.sleep(self.rate_limit_delay)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error collecting news for {symbol}: {e}\")\n",
    "            \n",
    "        return articles\n",
    "    \n",
    "    def filter_relevant_articles(self, articles: List[Dict], symbol: str) -> List[Dict]:\n",
    "        \"\"\"Filter articles for relevance to the stock.\"\"\"\n",
    "        filtered_articles = []\n",
    "        keywords = STOCK_KEYWORDS.get(symbol, [])\n",
    "        \n",
    "        for article in articles:\n",
    "            title = article.get('title', '').lower()\n",
    "            description = article.get('description', '').lower() if article.get('description') else ''\n",
    "            \n",
    "            text_content = f\"{title} {description}\"\n",
    "            \n",
    "            # Check relevance\n",
    "            relevance_score = 0\n",
    "            if symbol.lower() in text_content:\n",
    "                relevance_score += 3\n",
    "                \n",
    "            for keyword in keywords:\n",
    "                if keyword.lower() in text_content:\n",
    "                    relevance_score += 1\n",
    "            \n",
    "            # Keep articles with minimum relevance\n",
    "            if relevance_score >= 2:\n",
    "                article['relevance_score'] = relevance_score\n",
    "                article['symbol'] = symbol\n",
    "                article['processed_text'] = f\"{title}. {description}\"\n",
    "                filtered_articles.append(article)\n",
    "                \n",
    "        print(f\"Filtered to {len(filtered_articles)} relevant articles for {symbol}\")\n",
    "        return filtered_articles\n",
    "\n",
    "# Initialize news collector\n",
    "news_collector = NewsCollector(CONFIG['NEWS_API_KEY'])\n",
    "print(\"News collector initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea68646b",
   "metadata": {},
   "source": [
    "## 4. Financial Data Extraction\n",
    "\n",
    "Use yfinance to collect historical stock data including prices, volume, and technical indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a182602",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockDataCollector:\n",
    "    \"\"\"Collect stock price data and calculate technical indicators.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def collect_stock_data(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:\n",
    "        \"\"\"Collect stock price data for a symbol.\"\"\"\n",
    "        try:\n",
    "            print(f\"Collecting stock data for {symbol}...\")\n",
    "            \n",
    "            ticker = yf.Ticker(symbol)\n",
    "            data = ticker.history(start=start_date, end=end_date, interval='1d')\n",
    "            \n",
    "            if data.empty:\n",
    "                print(f\"No stock data found for {symbol}\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Reset index to make Date a column\n",
    "            data = data.reset_index()\n",
    "            data['Symbol'] = symbol\n",
    "            \n",
    "            print(f\"Collected {len(data)} days of stock data for {symbol}\")\n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error collecting stock data for {symbol}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def calculate_technical_indicators(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Calculate technical indicators for stock data.\"\"\"\n",
    "        if data.empty:\n",
    "            return data\n",
    "            \n",
    "        try:\n",
    "            # Simple Moving Average (20 days)\n",
    "            data['SMA_20'] = data['Close'].rolling(window=20).mean()\n",
    "            \n",
    "            # RSI (14 days)\n",
    "            delta = data['Close'].diff()\n",
    "            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "            rs = gain / loss\n",
    "            data['RSI_14'] = 100 - (100 / (1 + rs))\n",
    "            \n",
    "            # MACD\n",
    "            exp1 = data['Close'].ewm(span=12).mean()\n",
    "            exp2 = data['Close'].ewm(span=26).mean()\n",
    "            data['MACD'] = exp1 - exp2\n",
    "            data['MACD_Signal'] = data['MACD'].ewm(span=9).mean()\n",
    "            \n",
    "            # Bollinger Bands\n",
    "            data['BB_Middle'] = data['Close'].rolling(window=20).mean()\n",
    "            bb_std = data['Close'].rolling(window=20).std()\n",
    "            data['BB_Upper'] = data['BB_Middle'] + (bb_std * 2)\n",
    "            data['BB_Lower'] = data['BB_Middle'] - (bb_std * 2)\n",
    "            \n",
    "            # Volume indicators\n",
    "            data['Volume_SMA_20'] = data['Volume'].rolling(window=20).mean()\n",
    "            \n",
    "            # Volatility (20-day rolling standard deviation of returns)\n",
    "            data['Returns'] = data['Close'].pct_change()\n",
    "            data['Volatility_20'] = data['Returns'].rolling(window=20).std()\n",
    "            \n",
    "            # Price change indicators\n",
    "            data['Price_Change'] = data['Close'].diff()\n",
    "            data['Price_Change_Pct'] = data['Close'].pct_change()\n",
    "            \n",
    "            print(f\"Calculated technical indicators for {data['Symbol'].iloc[0]}\")\n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating technical indicators: {e}\")\n",
    "            return data\n",
    "\n",
    "# Initialize stock data collector\n",
    "stock_collector = StockDataCollector()\n",
    "print(\"Stock data collector initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d04cda",
   "metadata": {},
   "source": [
    "## 5. Data Collection Execution\n",
    "\n",
    "Now let's collect the actual data for our prototype using the past 30 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d043b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data for all symbols\n",
    "all_news_data = []\n",
    "all_stock_data = []\n",
    "\n",
    "print(\"Starting data collection for prototype...\")\n",
    "print(f\"Date range: {CONFIG['PROTOTYPE_START_DATE']} to {CONFIG['PROTOTYPE_END_DATE']}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for symbol in CONFIG['STOCK_SYMBOLS']:\n",
    "    print(f\"\\nProcessing {symbol}...\")\n",
    "    \n",
    "    # Collect news data\n",
    "    try:\n",
    "        news_articles = news_collector.collect_news_for_stock(\n",
    "            symbol, \n",
    "            CONFIG['PROTOTYPE_START_DATE'], \n",
    "            CONFIG['PROTOTYPE_END_DATE']\n",
    "        )\n",
    "        \n",
    "        # Filter relevant articles\n",
    "        relevant_articles = news_collector.filter_relevant_articles(news_articles, symbol)\n",
    "        all_news_data.extend(relevant_articles)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to collect news for {symbol}: {e}\")\n",
    "    \n",
    "    # Collect stock data\n",
    "    try:\n",
    "        stock_data = stock_collector.collect_stock_data(\n",
    "            symbol, \n",
    "            CONFIG['PROTOTYPE_START_DATE'], \n",
    "            CONFIG['PROTOTYPE_END_DATE']\n",
    "        )\n",
    "        \n",
    "        if not stock_data.empty:\n",
    "            stock_data = stock_collector.calculate_technical_indicators(stock_data)\n",
    "            all_stock_data.append(stock_data)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to collect stock data for {symbol}: {e}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"Data collection completed!\")\n",
    "print(f\"Total news articles collected: {len(all_news_data)}\")\n",
    "print(f\"Total stock symbols with data: {len(all_stock_data)}\")\n",
    "\n",
    "# Create DataFrames\n",
    "if all_news_data:\n",
    "    news_df = pd.DataFrame(all_news_data)\n",
    "    print(f\"News DataFrame shape: {news_df.shape}\")\n",
    "else:\n",
    "    news_df = pd.DataFrame()\n",
    "    print(\"No news data collected\")\n",
    "\n",
    "if all_stock_data:\n",
    "    stock_df = pd.concat(all_stock_data, ignore_index=True)\n",
    "    print(f\"Stock DataFrame shape: {stock_df.shape}\")\n",
    "else:\n",
    "    stock_df = pd.DataFrame()\n",
    "    print(\"No stock data collected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6e3818",
   "metadata": {},
   "source": [
    "## 6. Data Exploration and Visualization\n",
    "\n",
    "Let's explore the collected data to understand its structure and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb5b4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore news data\n",
    "if not news_df.empty:\n",
    "    print(\"NEWS DATA ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total articles: {len(news_df)}\")\n",
    "    print(f\"Date range: {news_df['publishedAt'].min()} to {news_df['publishedAt'].max()}\")\n",
    "    print(f\"Symbols covered: {news_df['symbol'].value_counts()}\")\n",
    "    print(f\"Average relevance score: {news_df['relevance_score'].mean():.2f}\")\n",
    "    \n",
    "    # News by symbol\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Articles per symbol\n",
    "    news_df['symbol'].value_counts().plot(kind='bar', ax=axes[0])\n",
    "    axes[0].set_title('News Articles per Symbol')\n",
    "    axes[0].set_xlabel('Symbol')\n",
    "    axes[0].set_ylabel('Article Count')\n",
    "    \n",
    "    # Relevance score distribution\n",
    "    news_df['relevance_score'].hist(bins=10, ax=axes[1])\n",
    "    axes[1].set_title('Relevance Score Distribution')\n",
    "    axes[1].set_xlabel('Relevance Score')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show sample articles\n",
    "    print(\"\\\\nSample news articles:\")\n",
    "    for i, row in news_df.head(3).iterrows():\n",
    "        print(f\"\\\\n{row['symbol']}: {row['title']}\")\n",
    "        print(f\"Published: {row['publishedAt']}\")\n",
    "        print(f\"Relevance: {row['relevance_score']}\")\n",
    "else:\n",
    "    print(\"No news data to analyze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d5ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore stock data\n",
    "if not stock_df.empty:\n",
    "    print(\"STOCK DATA ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total records: {len(stock_df)}\")\n",
    "    print(f\"Symbols: {stock_df['Symbol'].unique()}\")\n",
    "    print(f\"Date range: {stock_df['Date'].min()} to {stock_df['Date'].max()}\")\n",
    "    \n",
    "    # Stock price trends\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Stock Prices', 'Volume', 'RSI', 'Volatility'),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "    \n",
    "    for i, symbol in enumerate(stock_df['Symbol'].unique()):\n",
    "        symbol_data = stock_df[stock_df['Symbol'] == symbol]\n",
    "        color = colors[i % len(colors)]\n",
    "        \n",
    "        # Stock prices\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=symbol_data['Date'], y=symbol_data['Close'], \n",
    "                      name=f'{symbol} Close', line=dict(color=color)),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Volume\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=symbol_data['Date'], y=symbol_data['Volume'], \n",
    "                      name=f'{symbol} Volume', line=dict(color=color)),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # RSI\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=symbol_data['Date'], y=symbol_data['RSI_14'], \n",
    "                      name=f'{symbol} RSI', line=dict(color=color)),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Volatility\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=symbol_data['Date'], y=symbol_data['Volatility_20'], \n",
    "                      name=f'{symbol} Vol', line=dict(color=color)),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(height=800, title_text=\"Stock Data Overview\")\n",
    "    fig.show()\n",
    "    \n",
    "    # Statistical summary\n",
    "    print(\"\\\\nStock price statistics:\")\n",
    "    print(stock_df.groupby('Symbol')['Close'].agg(['min', 'max', 'mean', 'std']).round(2))\n",
    "else:\n",
    "    print(\"No stock data to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaefb68",
   "metadata": {},
   "source": [
    "## 7. News Embedding with FinBERT\n",
    "\n",
    "Load FinBERT model and create embeddings for news articles to capture sentiment and meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81b2ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsEmbedder:\n",
    "    \"\"\"Create embeddings for news articles using FinBERT.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'ProsusAI/finbert'):\n",
    "        print(f\"Loading FinBERT model: {model_name}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Move to GPU if available\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(f\"FinBERT model loaded on {self.device}\")\n",
    "    \n",
    "    def embed_texts(self, texts: List[str], batch_size: int = 16) -> np.ndarray:\n",
    "        \"\"\"Create embeddings for a list of texts.\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        print(f\"Creating embeddings for {len(texts)} texts...\")\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = self.tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors='pt'\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # Get embeddings\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                # Use [CLS] token embedding\n",
    "                batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "                embeddings.append(batch_embeddings)\n",
    "        \n",
    "        embeddings = np.vstack(embeddings)\n",
    "        print(f\"Created embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "# Initialize news embedder (this may take a while to download the model)\n",
    "if not news_df.empty:\n",
    "    print(\"Initializing FinBERT for news embedding...\")\n",
    "    news_embedder = NewsEmbedder(CONFIG['FINBERT_MODEL'])\n",
    "else:\n",
    "    print(\"Skipping FinBERT initialization - no news data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1207edc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create news embeddings\n",
    "if not news_df.empty and 'news_embedder' in locals():\n",
    "    print(\"Creating news embeddings...\")\n",
    "    \n",
    "    # Extract processed texts\n",
    "    news_texts = news_df['processed_text'].fillna('').tolist()\n",
    "    \n",
    "    # Create embeddings\n",
    "    news_embeddings = news_embedder.embed_texts(news_texts, batch_size=8)\n",
    "    \n",
    "    # Add embeddings to DataFrame\n",
    "    for i in range(news_embeddings.shape[1]):\n",
    "        news_df[f'embedding_{i}'] = news_embeddings[:, i]\n",
    "    \n",
    "    print(f\"News embeddings created: {news_embeddings.shape}\")\n",
    "    print(f\"News DataFrame now has {len(news_df.columns)} columns\")\n",
    "    \n",
    "    # Analyze embedding statistics\n",
    "    embedding_cols = [col for col in news_df.columns if col.startswith('embedding_')]\n",
    "    embedding_stats = news_df[embedding_cols].describe()\n",
    "    \n",
    "    print(\"\\\\nEmbedding statistics:\")\n",
    "    print(f\"Mean embedding magnitude: {np.mean(np.linalg.norm(news_embeddings, axis=1)):.4f}\")\n",
    "    print(f\"Embedding dimensions: {len(embedding_cols)}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Skipping news embedding creation - no news data or embedder available\")\n",
    "    news_embeddings = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79826300",
   "metadata": {},
   "source": [
    "## 8. Feature Engineering and Dataset Creation\n",
    "\n",
    "Combine news embeddings with financial indicators to create training datasets with proper time alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c8bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineering:\n",
    "    \"\"\"Create features for training by combining news and stock data.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def create_training_dataset(self, stock_df: pd.DataFrame, news_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create training dataset with proper time alignment.\"\"\"\n",
    "        if stock_df.empty:\n",
    "            print(\"No stock data available for feature engineering\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        print(\"Creating training dataset...\")\n",
    "        \n",
    "        # Convert dates\n",
    "        stock_df['Date'] = pd.to_datetime(stock_df['Date'])\n",
    "        if not news_df.empty:\n",
    "            news_df['publishedAt'] = pd.to_datetime(news_df['publishedAt'])\n",
    "        \n",
    "        training_data = []\n",
    "        \n",
    "        for symbol in stock_df['Symbol'].unique():\n",
    "            symbol_stock_data = stock_df[stock_df['Symbol'] == symbol].sort_values('Date')\n",
    "            symbol_news_data = news_df[news_df['symbol'] == symbol] if not news_df.empty else pd.DataFrame()\n",
    "            \n",
    "            print(f\"Processing {symbol}: {len(symbol_stock_data)} stock records, {len(symbol_news_data)} news articles\")\n",
    "            \n",
    "            # Create features for each trading day\n",
    "            for idx, row in symbol_stock_data.iterrows():\n",
    "                current_date = row['Date']\n",
    "                \n",
    "                # Skip if we don't have enough historical data\n",
    "                if idx < 20:  # Need 20 days for technical indicators\n",
    "                    continue\n",
    "                \n",
    "                # Create feature vector\n",
    "                features = {}\n",
    "                features['Symbol'] = symbol\n",
    "                features['Date'] = current_date\n",
    "                features['Target_Date'] = current_date + pd.Timedelta(days=7)\n",
    "                \n",
    "                # Financial features (5 key indicators)\n",
    "                features['Close_Price'] = row['Close']\n",
    "                features['SMA_20'] = row.get('SMA_20', 0)\n",
    "                features['RSI_14'] = row.get('RSI_14', 50)\n",
    "                features['MACD'] = row.get('MACD', 0)\n",
    "                features['Volatility_20'] = row.get('Volatility_20', 0)\n",
    "                \n",
    "                # News features (aggregate embeddings from past 7 days)\n",
    "                if not symbol_news_data.empty:\n",
    "                    start_date = current_date - pd.Timedelta(days=7)\n",
    "                    relevant_news = symbol_news_data[\n",
    "                        (symbol_news_data['publishedAt'] >= start_date) & \n",
    "                        (symbol_news_data['publishedAt'] <= current_date)\n",
    "                    ]\n",
    "                    \n",
    "                    if len(relevant_news) > 0:\n",
    "                        # Aggregate news embeddings (mean)\n",
    "                        embedding_cols = [col for col in relevant_news.columns if col.startswith('embedding_')]\n",
    "                        if embedding_cols:\n",
    "                            news_embedding = relevant_news[embedding_cols].mean().values\n",
    "                            for i, emb_val in enumerate(news_embedding):\n",
    "                                features[f'news_emb_{i}'] = emb_val\n",
    "                        \n",
    "                        features['news_count'] = len(relevant_news)\n",
    "                        features['avg_relevance'] = relevant_news['relevance_score'].mean()\n",
    "                    else:\n",
    "                        # No news - use zero embeddings\n",
    "                        for i in range(768):  # FinBERT embedding dimension\n",
    "                            features[f'news_emb_{i}'] = 0.0\n",
    "                        features['news_count'] = 0\n",
    "                        features['avg_relevance'] = 0\n",
    "                else:\n",
    "                    # No news data - use zero embeddings\n",
    "                    for i in range(768):\n",
    "                        features[f'news_emb_{i}'] = 0.0\n",
    "                    features['news_count'] = 0\n",
    "                    features['avg_relevance'] = 0\n",
    "                \n",
    "                # Target: stock price 7 days later\n",
    "                target_date = current_date + pd.Timedelta(days=7)\n",
    "                future_data = symbol_stock_data[symbol_stock_data['Date'] >= target_date]\n",
    "                \n",
    "                if len(future_data) > 0:\n",
    "                    features['Target_Price'] = future_data['Close'].iloc[0]\n",
    "                    features['Price_Change'] = features['Target_Price'] - features['Close_Price']\n",
    "                    features['Price_Change_Pct'] = (features['Price_Change'] / features['Close_Price']) * 100\n",
    "                    \n",
    "                    training_data.append(features)\n",
    "        \n",
    "        if training_data:\n",
    "            training_df = pd.DataFrame(training_data)\n",
    "            print(f\"Created training dataset with {len(training_df)} records\")\n",
    "            print(f\"Features: {len(training_df.columns)} columns\")\n",
    "            return training_df\n",
    "        else:\n",
    "            print(\"No training data created\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "# Create feature engineering instance and training dataset\n",
    "feature_engineer = FeatureEngineering()\n",
    "training_df = feature_engineer.create_training_dataset(stock_df, news_df)\n",
    "\n",
    "if not training_df.empty:\n",
    "    print(f\"\\\\nTraining dataset summary:\")\n",
    "    print(f\"Shape: {training_df.shape}\")\n",
    "    print(f\"Date range: {training_df['Date'].min()} to {training_df['Date'].max()}\")\n",
    "    print(f\"Symbols: {training_df['Symbol'].value_counts()}\")\n",
    "    \n",
    "    # Remove rows with missing targets\n",
    "    training_df = training_df.dropna(subset=['Target_Price'])\n",
    "    print(f\"After removing missing targets: {training_df.shape}\")\n",
    "else:\n",
    "    print(\"No training dataset created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ac7b18",
   "metadata": {},
   "source": [
    "## 9. Model Architecture and Training\n",
    "\n",
    "Create a PyTorch MLP model that combines news embeddings and financial features for price prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b5bd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockPredictionMLP(nn.Module):\n",
    "    \"\"\"Multi-layer perceptron for stock price prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int] = [512, 256, 128], dropout_rate: float = 0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    \"\"\"PyTorch dataset for stock prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, features: np.ndarray, targets: np.ndarray):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.targets = torch.FloatTensor(targets)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs: int = 20, learning_rate: float = 0.001):\n",
    "    \"\"\"Train the stock prediction model.\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    print(f\"Training on {device}\")\n",
    "    print(f\"Training samples: {len(train_loader.dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch_features, batch_targets in train_loader:\n",
    "            batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_features).squeeze()\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_features, batch_targets in val_loader:\n",
    "                batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
    "                outputs = model(batch_features).squeeze()\n",
    "                loss = criterion(outputs, batch_targets)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "print(\"Model classes defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a3f9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "if not training_df.empty:\n",
    "    print(\"Preparing data for training...\")\n",
    "    \n",
    "    # Separate features and targets\n",
    "    feature_cols = [col for col in training_df.columns if col not in ['Symbol', 'Date', 'Target_Date', 'Target_Price', 'Price_Change', 'Price_Change_Pct']]\n",
    "    \n",
    "    X = training_df[feature_cols].values\n",
    "    y = training_df['Target_Price'].values\n",
    "    \n",
    "    print(f\"Feature matrix shape: {X.shape}\")\n",
    "    print(f\"Target vector shape: {y.shape}\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    X = np.nan_to_num(X, nan=0.0)\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler_X = StandardScaler()\n",
    "    X_scaled = scaler_X.fit_transform(X)\n",
    "    \n",
    "    # Split data temporally (80% train, 20% validation)\n",
    "    split_idx = int(0.8 * len(X_scaled))\n",
    "    X_train, X_val = X_scaled[:split_idx], X_scaled[split_idx:]\n",
    "    y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "    \n",
    "    # Create datasets and data loaders\n",
    "    train_dataset = StockDataset(X_train, y_train)\n",
    "    val_dataset = StockDataset(X_val, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    input_dim = X_scaled.shape[1]\n",
    "    model = StockPredictionMLP(input_dim=input_dim)\n",
    "    \n",
    "    print(f\"Model initialized with input dimension: {input_dim}\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\\\nStarting training...\")\n",
    "    train_losses, val_losses = train_model(\n",
    "        model, train_loader, val_loader, \n",
    "        num_epochs=CONFIG['EPOCHS'], \n",
    "        learning_rate=CONFIG['LEARNING_RATE']\n",
    "    )\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "    \n",
    "else:\n",
    "    print(\"No training data available for model training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911e7969",
   "metadata": {},
   "source": [
    "## 10. Model Evaluation and Visualization\n",
    "\n",
    "Evaluate the trained model and visualize predictions vs actual prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1e95be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance\n",
    "if not training_df.empty and 'model' in locals():\n",
    "    print(\"Evaluating model performance...\")\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.eval()\n",
    "    \n",
    "    # Make predictions on validation set\n",
    "    with torch.no_grad():\n",
    "        val_predictions = []\n",
    "        val_actuals = []\n",
    "        \n",
    "        for batch_features, batch_targets in val_loader:\n",
    "            batch_features = batch_features.to(device)\n",
    "            outputs = model(batch_features).squeeze()\n",
    "            val_predictions.extend(outputs.cpu().numpy())\n",
    "            val_actuals.extend(batch_targets.numpy())\n",
    "    \n",
    "    val_predictions = np.array(val_predictions)\n",
    "    val_actuals = np.array(val_actuals)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(val_actuals, val_predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(val_actuals, val_predictions))\n",
    "    r2 = r2_score(val_actuals, val_predictions)\n",
    "    \n",
    "    # Calculate directional accuracy\n",
    "    actual_direction = np.sign(np.diff(val_actuals))\n",
    "    pred_direction = np.sign(np.diff(val_predictions))\n",
    "    directional_accuracy = np.mean(actual_direction == pred_direction) * 100\n",
    "    \n",
    "    print(f\"\\\\nModel Performance Metrics:\")\n",
    "    print(f\"Mean Absolute Error (MAE): ${mae:.2f}\")\n",
    "    print(f\"Root Mean Square Error (RMSE): ${rmse:.2f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    print(f\"Directional Accuracy: {directional_accuracy:.2f}%\")\n",
    "    \n",
    "    # Plot training curves\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Training/Validation Loss\n",
    "    axes[0].plot(train_losses, label='Training Loss', color='blue')\n",
    "    axes[0].plot(val_losses, label='Validation Loss', color='red')\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Predictions vs Actuals\n",
    "    axes[1].scatter(val_actuals, val_predictions, alpha=0.6, color='blue')\n",
    "    axes[1].plot([val_actuals.min(), val_actuals.max()], [val_actuals.min(), val_actuals.max()], 'r--', lw=2)\n",
    "    axes[1].set_xlabel('Actual Prices')\n",
    "    axes[1].set_ylabel('Predicted Prices')\n",
    "    axes[1].set_title(f'Predictions vs Actuals (R² = {r2:.4f})')\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot time series predictions\n",
    "    if len(val_predictions) > 10:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        indices = range(len(val_predictions))\n",
    "        plt.plot(indices, val_actuals, label='Actual', color='blue', linewidth=2)\n",
    "        plt.plot(indices, val_predictions, label='Predicted', color='red', linewidth=2, alpha=0.7)\n",
    "        plt.title('Time Series: Actual vs Predicted Stock Prices')\n",
    "        plt.xlabel('Time Index')\n",
    "        plt.ylabel('Stock Price ($)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No trained model available for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a514234",
   "metadata": {},
   "source": [
    "## 11. Data Storage and Cloud Integration\n",
    "\n",
    "Save processed data locally and demonstrate Google Cloud Storage integration for scalability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebae16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory\n",
    "import os\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "print(\"Created local data directories\")\n",
    "\n",
    "# Save data locally\n",
    "if not news_df.empty:\n",
    "    news_file = 'data/raw/prototype_news_data.parquet'\n",
    "    news_df.to_parquet(news_file, index=False)\n",
    "    print(f\"Saved news data: {news_file} ({os.path.getsize(news_file) / 1024:.1f} KB)\")\n",
    "\n",
    "if not stock_df.empty:\n",
    "    stock_file = 'data/raw/prototype_stock_data.parquet'\n",
    "    stock_df.to_parquet(stock_file, index=False)\n",
    "    print(f\"Saved stock data: {stock_file} ({os.path.getsize(stock_file) / 1024:.1f} KB)\")\n",
    "\n",
    "if not training_df.empty:\n",
    "    training_file = 'data/processed/prototype_training_data.parquet'\n",
    "    training_df.to_parquet(training_file, index=False)\n",
    "    print(f\"Saved training data: {training_file} ({os.path.getsize(training_file) / 1024:.1f} KB)\")\n",
    "\n",
    "# Save model\n",
    "if 'model' in locals():\n",
    "    model_file = 'models/prototype_model.pth'\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'scaler': scaler_X,\n",
    "        'feature_columns': feature_cols,\n",
    "        'model_config': {\n",
    "            'input_dim': input_dim,\n",
    "            'hidden_dims': [512, 256, 128],\n",
    "            'dropout_rate': 0.3\n",
    "        }\n",
    "    }, model_file)\n",
    "    print(f\"Saved model: {model_file} ({os.path.getsize(model_file) / 1024:.1f} KB)\")\n",
    "\n",
    "print(\"\\\\nLocal data saving completed!\")\n",
    "\n",
    "# Demonstrate GCS integration (if available)\n",
    "if GCS_AVAILABLE and 'google.colab' not in sys.modules:\n",
    "    print(\"\\\\n\" + \"=\"*50)\n",
    "    print(\"GOOGLE CLOUD STORAGE INTEGRATION\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"To enable GCS integration:\")\n",
    "    print(\"1. Set up GCP project and enable Cloud Storage API\")\n",
    "    print(\"2. Create a service account and download credentials\")\n",
    "    print(\"3. Save credentials as 'gcp-credentials.json'\")\n",
    "    print(\"4. Update CONFIG['GCP_PROJECT_ID'] and CONFIG['GCP_BUCKET_NAME']\")\n",
    "    print(\"5. Uncomment the code below to upload data to GCS\")\n",
    "    \n",
    "    # Uncomment this section when GCS is properly configured\n",
    "    # try:\n",
    "    #     from google.cloud import storage\n",
    "    #     \n",
    "    #     # Initialize GCS client\n",
    "    #     client = storage.Client(project=CONFIG['GCP_PROJECT_ID'])\n",
    "    #     bucket = client.bucket(CONFIG['GCP_BUCKET_NAME'])\n",
    "    #     \n",
    "    #     # Upload files\n",
    "    #     files_to_upload = [\n",
    "    #         ('data/raw/prototype_news_data.parquet', 'raw_data/prototype_news_data.parquet'),\n",
    "    #         ('data/raw/prototype_stock_data.parquet', 'raw_data/prototype_stock_data.parquet'),\n",
    "    #         ('data/processed/prototype_training_data.parquet', 'processed_data/prototype_training_data.parquet'),\n",
    "    #         ('models/prototype_model.pth', 'models/prototype_model.pth')\n",
    "    #     ]\n",
    "    #     \n",
    "    #     for local_file, gcs_path in files_to_upload:\n",
    "    #         if os.path.exists(local_file):\n",
    "    #             blob = bucket.blob(gcs_path)\n",
    "    #             blob.upload_from_filename(local_file)\n",
    "    #             print(f\"Uploaded {local_file} to GCS: gs://{CONFIG['GCP_BUCKET_NAME']}/{gcs_path}\")\n",
    "    #     \n",
    "    #     print(\"GCS upload completed!\")\n",
    "    #     \n",
    "    # except Exception as e:\n",
    "    #     print(f\"GCS upload failed: {e}\")\n",
    "    #     print(\"Make sure GCP credentials are properly configured\")\n",
    "\n",
    "else:\n",
    "    print(\"\\\\nGCS integration not available (running in Colab or missing dependencies)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e63104",
   "metadata": {},
   "source": [
    "## 12. Cost Monitoring and Next Steps\n",
    "\n",
    "Implement cost monitoring strategies and outline next steps for full system deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74930286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate resource usage and cost estimates\n",
    "print(\"RESOURCE USAGE AND COST ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Data usage\n",
    "total_data_size = 0\n",
    "data_files = [\n",
    "    'data/raw/prototype_news_data.parquet',\n",
    "    'data/raw/prototype_stock_data.parquet', \n",
    "    'data/processed/prototype_training_data.parquet',\n",
    "    'models/prototype_model.pth'\n",
    "]\n",
    "\n",
    "for file_path in data_files:\n",
    "    if os.path.exists(file_path):\n",
    "        size_kb = os.path.getsize(file_path) / 1024\n",
    "        total_data_size += size_kb\n",
    "        print(f\"{file_path}: {size_kb:.1f} KB\")\n",
    "\n",
    "print(f\"\\\\nTotal data size: {total_data_size:.1f} KB ({total_data_size/1024:.2f} MB)\")\n",
    "\n",
    "# Estimate full-scale costs\n",
    "print(\"\\\\nFULL-SCALE COST ESTIMATES (1 year of data)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# NewsAPI costs\n",
    "daily_articles_per_stock = 50\n",
    "total_stocks = len(CONFIG['STOCK_SYMBOLS'])\n",
    "days_per_year = 365\n",
    "total_api_calls = daily_articles_per_stock * total_stocks * days_per_year\n",
    "\n",
    "print(f\"NewsAPI Usage:\")\n",
    "print(f\"- Articles per stock per day: {daily_articles_per_stock}\")\n",
    "print(f\"- Total stocks: {total_stocks}\")\n",
    "print(f\"- Total API calls per year: {total_api_calls:,}\")\n",
    "print(f\"- Cost: FREE (within 1000 requests/day limit)\")\n",
    "\n",
    "# GCS storage costs\n",
    "estimated_full_data_size_gb = (total_data_size * 365) / (1024 * 1024)  # Scale up for full year\n",
    "gcs_storage_cost_per_gb_per_month = 0.020  # Standard storage\n",
    "gcs_monthly_cost = estimated_full_data_size_gb * gcs_storage_cost_per_gb_per_month\n",
    "\n",
    "print(f\"\\\\nGoogle Cloud Storage:\")\n",
    "print(f\"- Estimated full dataset size: {estimated_full_data_size_gb:.2f} GB\")\n",
    "print(f\"- Monthly storage cost: ${gcs_monthly_cost:.2f}\")\n",
    "print(f\"- Annual storage cost: ${gcs_monthly_cost * 12:.2f}\")\n",
    "\n",
    "# Compute costs (training)\n",
    "print(f\"\\\\nCompute Costs (Training):\")\n",
    "print(f\"- Colab Pro: $9.99/month (100 compute units)\")\n",
    "print(f\"- Alternative: Kaggle (30 hours/week free GPU)\")\n",
    "print(f\"- GCP Compute Engine: ~$0.50/hour (preemptible GPU)\")\n",
    "\n",
    "# Total estimated costs\n",
    "total_monthly_cost = gcs_monthly_cost + 9.99  # GCS + Colab Pro\n",
    "print(f\"\\\\nESTIMATED TOTAL MONTHLY COST: ${total_monthly_cost:.2f}\")\n",
    "print(f\"ESTIMATED TOTAL ANNUAL COST: ${total_monthly_cost * 12:.2f}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"COST OPTIMIZATION STRATEGIES\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Use free tiers extensively:\")\n",
    "print(\"   - GCP Free Tier: $300 credit + always-free resources\")\n",
    "print(\"   - Colab Free: Limited but sufficient for prototyping\")\n",
    "print(\"   - NewsAPI Free: 1000 requests/day\")\n",
    "\n",
    "print(\"\\\\n2. Efficient data processing:\")\n",
    "print(\"   - Batch process news embeddings\")\n",
    "print(\"   - Use parquet format for compression\")\n",
    "print(\"   - Implement data partitioning by date/symbol\")\n",
    "\n",
    "print(\"\\\\n3. Smart resource usage:\")\n",
    "print(\"   - Use spot/preemptible instances\")\n",
    "print(\"   - Scale compute resources dynamically\") \n",
    "print(\"   - Cache processed embeddings\")\n",
    "\n",
    "print(\"\\\\n4. Set up billing alerts:\")\n",
    "print(\"   - Daily spending alerts\")\n",
    "print(\"   - Budget limits with automatic shutdowns\")\n",
    "print(\"   - Monitor API usage quotas\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"NEXT STEPS FOR FULL IMPLEMENTATION\")\n",
    "print(\"=\"*60)\n",
    "print(\"Week 2-3: Full Data Collection & Model Training\")\n",
    "print(\"- Extend date range to full year (Sep 2024 - Aug 2025)\")\n",
    "print(\"- Implement parallel processing for multiple stocks\")\n",
    "print(\"- Scale up to 10-20 stocks for better diversification\")\n",
    "print(\"- Implement advanced hyperparameter tuning with Optuna\")\n",
    "print(\"- Add more sophisticated feature engineering\")\n",
    "\n",
    "print(\"\\\\nWeek 3-4: Deployment & Monitoring\")\n",
    "print(\"- Create FastAPI wrapper for model serving\")\n",
    "print(\"- Deploy to Cloud Run or Hugging Face Spaces\")\n",
    "print(\"- Build Streamlit dashboard for visualization\")\n",
    "print(\"- Implement real-time prediction pipeline\")\n",
    "print(\"- Set up automated model retraining\")\n",
    "\n",
    "print(\"\\\\nWeek 4+: Production & Optimization\")\n",
    "print(\"- Implement A/B testing for model versions\")\n",
    "print(\"- Add more data sources (Twitter, Reddit, SEC filings)\")\n",
    "print(\"- Implement ensemble methods\")\n",
    "print(\"- Create backtesting framework\")\n",
    "print(\"- Add risk management features\")\n",
    "\n",
    "print(f\"\\\\n{'='*60}\")\n",
    "print(\"PROTOTYPE COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
